# <center>Design Diary - 2/26/2020 - Web Scraper</center>

For this project the assignment was to scrape a website for informaton. Originally, my idea for this project was to scrape craigslist for listings and cross reference the cars I found with Kelley Blue Book. I really wanted to do this, however I quickly found out that Blue Book would be almost impossible to scrape without using selenium to open a window for each listing and have it automatically select the basic trim option for each car or paying for their API. Because of this, I decided to scrape a seprate website, [Santander Consumer USA](https://santanderconsumerusa.com/blog/25-vehicles-that-hold-value-best-over-five-years-iseecars-com), for 25 cars that hold ther value best over time. After this, the program scrapes Craigslist (default search area is Humboldt) and writes all listing details of each car into 25 seprate CSV files, and has a seprate function that filters through the CSVs to find cars with an annual mileage under a target amount (Cars without a mileage listed are thrown out). From here, I wanted a way to display the filtered car information in a clean and elegant way. For this, I decided to use Flask to display everything on a webpage and give the user an option where they can create their own searches in new areas and for different cars. I also wanted a way to refresh the default search and allows the user to choose a new area.

In this project alone, I feel like I learned a lot more than I have with any other project that I've made. I loved the freedom I had to go in any direction that I wanted, and I loved the process of problem solving and forcing myself to learn on my own. I also enjoyed pushing myself to use new frameworks and modules that I normally wouldn't have used, like Bootstrap and Flask-WTF (Form module for Flask). Probably the hardest part of this project was dealing with listings where the owner put up inaccurate or "bad" information that I had to fix or ignore. For example, some people would list their 10+ year old car as having 156 miles when in fact the car had 156,000 miles. Obviously, when calculating annual mileage, this gives very innaccurate results. I solved this using logic where I calculated the age of the car and how many miles the car had. If the car was greater than two years old and had less than 400 miles, I multiplied the mileage by 1000, and if it was less than 4000 miles and was older than 3 years, I multiplied the result by 100. Although this probably wasn't the most perfect fix, it worked, and was able to weed out a lot of bad results. The most fun part of this for me was designing the tying my original program into Flask, and making the output cleaner. Something I wanted to do but couldn't figure out, was pulling data from Kelley Blue Book, as it required a lot of opening browsers or a lot of money.

I also plan on extending this project by analying data from differnet areas and seeing which area is the cheapest, and which area has better prices for different makes of cars. I also want to possibly improve my scraping algorithm, as it currently runs at O(N^2) which doesn't seem like the most efficient algorithm I could use.